{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User routes on the site\n",
    "## Description\n",
    "**Clickstream** is a sequence of user actions on a website. It allows you to understand how users interact with the site. In this task, you need to find the most frequent custom routes.\n",
    "\n",
    "## Input data\n",
    "Input data is а table with clickstream data in file `hdfs:/data/clickstream.csv`.\n",
    "\n",
    "### Table structure\n",
    "* `user_id (int)` - Unique user identifier.\n",
    "* `session_id (int)` - Unique identifier for the user session. The user's session lasts until the identifier changes.\n",
    "* `event_type (string)` - Event type from the list:\n",
    "    * **page** - visit to the page\n",
    "    * **event** - any action on the page\n",
    "    * <b>&lt;custom&gt;</b> - string with any other type\n",
    "* `event_type (string)` - Page on the site.\n",
    "* `timestamp (int)` - Unix-timestamp of action.\n",
    "\n",
    "### Browser errors\n",
    "Errors can sometimes occur in the user's browser - after such an error appears, we can no longer trust the data of this session and all the following lines after the error or at the same time with it are considered corrupted and **should not be counted** in statistics.\n",
    "\n",
    "When an error occurs on the page, a random string containing the word **error** will be written to the `event_type` field.\n",
    "\n",
    "### Sample of user session\n",
    "<pre>\n",
    "+-------+----------+------------+----------+----------+\n",
    "|user_id|session_id|  event_type|event_page| timestamp|\n",
    "+-------+----------+------------+----------+----------+\n",
    "|    562|       507|        page|      main|1620494781|\n",
    "|    562|       507|       event|      main|1620494788|\n",
    "|    562|       507|       event|      main|1620494798|\n",
    "|    562|       507|        page|    family|1620494820|\n",
    "|    562|       507|       event|    family|1620494828|\n",
    "|    562|       507|        page|      main|1620494848|\n",
    "|    562|       507|wNaxLlerrorU|      main|1620494865|\n",
    "|    562|       507|       event|      main|1620494873|\n",
    "|    562|       507|        page|      news|1620494875|\n",
    "|    562|       507|        page|   tariffs|1620494876|\n",
    "|    562|       507|       event|   tariffs|1620494884|\n",
    "|    562|       514|        page|      main|1620728918|\n",
    "|    562|       514|       event|      main|1620729174|\n",
    "|    562|       514|        page|   archive|1620729674|\n",
    "|    562|       514|        page|     bonus|1620729797|\n",
    "|    562|       514|        page|   tariffs|1620731090|\n",
    "|    562|       514|       event|   tariffs|1620731187|\n",
    "+-------+----------+------------+----------+----------+\n",
    "</pre>\n",
    "\n",
    "#### Correct user routes for a given user:\n",
    "* **Session 507**: main-family-main\n",
    "* **Session 514**: main-archive-bonus-tariffs\n",
    "\n",
    "Route elements are ordered by the time they appear in the clickstream, from earliest to latest.\n",
    "\n",
    "The route must be accounted for completely before the end of the session or an error in the session.\n",
    "\n",
    "## Task\n",
    "You need to use the Spark SQL, Spark RDD and Spark DF interfaces to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site.\n",
    "\n",
    "Each line of the file should contain the `route` and `count` values **separated by tabs**, where:\n",
    "* `route` - route on the site, consisting of pages separated by \"-\".\n",
    "* `count` - the number of user sessions in which this route was.\n",
    "\n",
    "The lines must be **ordered in descending order** of the `count` field.\n",
    "\n",
    "## Criteria\n",
    "You can get maximum of 3.5 points (final grade) for this assignment, depedning on the number of interface you manage to leverage. The criteria are as follows:\n",
    "\n",
    "* 0.5 points – Spark SQL solution with 1 query\n",
    "* 0.5 points – Spark SQL solution with <=2 queries\n",
    "* 0.5 points – Spark RDD solution\n",
    "* 0.5 points – Spark DF solution\n",
    "* 0.5 points – your solution algorithm is relatively optimized, i.e.: no O^2 or O^3 complexities; appropriate object usage; no data leaks, code quality and readability (no single-letter variables, PEP8 compliant) etc. This is evaluated by staff.\n",
    "* 1 point – 1 on 1 screening session. During this session staff member can ask you questions regarding your solution logic, framework usage, questionable parts of your code etc. If your code is clean enough, the staff member can just ask you to solve a theoretical problem connected to Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-10-10 20:31:29,626 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, ShortType\n",
    "\n",
    "sc = SparkContext(appName=\"HDFS Homework\")\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark DF Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+----------+----------+\n",
      "|user_id|session_id|  event_type|event_page| timestamp|\n",
      "+-------+----------+------------+----------+----------+\n",
      "|    562|       507|        page|      main|1695584127|\n",
      "|    562|       507|       event|      main|1695584134|\n",
      "|    562|       507|       event|      main|1695584144|\n",
      "|    562|       507|       event|      main|1695584147|\n",
      "|    562|       507|wNaxLlerrorU|      main|1695584154|\n",
      "+-------+----------+------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading csv into dataframe\n",
    "\n",
    "df = spark.read.csv('hdfs:/data/clickstream.csv', header=True, sep='\\t', inferSchema=True)\n",
    "df.show(5) # check table structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for each user_id and session_id calculate the timestamp\n",
    "of first error if any and store it into dataframe\n",
    "\"\"\"\n",
    "\n",
    "error_ts_df = (df.filter(df.event_type.contains('error'))\n",
    "                 .groupBy('user_id', 'session_id')\n",
    "                 .agg({'timestamp': 'min'})\n",
    "                 .withColumnRenamed('min(timestamp)', 'error_ts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "join the error timestamp with the main dataframe\n",
    "\"\"\"\n",
    "\n",
    "df_join = df.join(error_ts_df, on=['user_id', 'session_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|              route|count|\n",
      "+-------------------+-----+\n",
      "|               main| 8090|\n",
      "|       main-archive| 1095|\n",
      "|        main-rabota| 1039|\n",
      "|      main-internet|  880|\n",
      "|         main-bonus|  865|\n",
      "|          main-news|  760|\n",
      "|       main-tariffs|  669|\n",
      "|        main-online|  584|\n",
      "|         main-vklad|  514|\n",
      "|main-archive-rabota|  167|\n",
      "+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "calculate the route with the count\n",
    "\"\"\"\n",
    "\n",
    "# filter the redundat rows based on error timestamp and type of event type\n",
    "df_output = (df_join.filter((F.col('event_type') == 'page') & ((F.col('error_ts').isNull()) | (F.col('timestamp') < F.col('error_ts'))))\n",
    "                    .orderBy('user_id', 'session_id', 'timestamp') # order by timestamp before creating route\n",
    "                    .groupBy('user_id', 'session_id') # create route of unique user_id + session_id\n",
    "                    .agg(F.concat_ws('-', F.collect_list(F.col('event_page')))) # create route by concatenating string\n",
    "                    .withColumnRenamed('concat_ws(-, collect_list(event_page))', 'route') # rename colum to route\n",
    "                    .groupBy('route') # group by route\n",
    "                    .count() # count of routes\n",
    "                    .sort(F.col('count').desc()) # sort in decending order\n",
    "             )\n",
    "\n",
    "df_output.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write results\n",
    "(df_output.limit(30)\n",
    "          .select(F.concat_ws(',', *df_output.columns))\n",
    "          .write.option('header', False)\n",
    "          .text(\"df_result\", lineSep=\"\\t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+----------+\n",
      "|user_id|session_id|event_type|event_page| timestamp|\n",
      "+-------+----------+----------+----------+----------+\n",
      "|   4645|       935|      page|   archive|1696270588|\n",
      "|   2251|       378|      page|    rabota|1696270591|\n",
      "|   2222|       704|      page|    rabota|1696270600|\n",
      "+-------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read csv and register a temprory table\n",
    "\n",
    "df = spark.read.csv('hdfs:/data/clickstream.csv', header=True, sep='\\t', inferSchema=True)\n",
    "df.registerTempTable('clickstream')\n",
    "df.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|              route|count|\n",
      "+-------------------+-----+\n",
      "|               main| 8090|\n",
      "|       main-archive| 1095|\n",
      "|        main-rabota| 1039|\n",
      "|      main-internet|  879|\n",
      "|         main-bonus|  865|\n",
      "|          main-news|  760|\n",
      "|       main-tariffs|  669|\n",
      "|        main-online|  584|\n",
      "|         main-vklad|  514|\n",
      "|main-archive-rabota|  167|\n",
      "+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "'''\n",
    "this query does exactly the same thing as we did in dataframe solution\n",
    "innermost select query calculates the error timestamp\n",
    "2nd select query concatenates page to create route for each user + session_id\n",
    "3rd query just count the routes\n",
    "'''\n",
    "\n",
    "sql_out = spark.sql(\n",
    "            '''\n",
    "            SELECT route, COUNT(*) AS count\n",
    "            FROM (SELECT concat_ws('-', sort_array(collect_list(struct(timestamp, event_page))).event_page) as route\n",
    "                  FROM clickstream\n",
    "                  LEFT JOIN (SELECT user_id, session_id, MIN(timestamp) as error_ts\n",
    "                             FROM clickstream\n",
    "                             WHERE event_type LIKE '%error%'\n",
    "                             GROUP BY user_id, session_id) AS error_ts_table\n",
    "                  ON clickstream.user_id = error_ts_table.user_id AND clickstream.session_id = error_ts_table.session_id\n",
    "                  WHERE (event_type = 'page') AND (error_ts IS NULL OR timestamp < error_ts)\n",
    "                  GROUP BY clickstream.user_id, clickstream.session_id)\n",
    "            GROUP BY route\n",
    "            ORDER BY count DESC\n",
    "            '''\n",
    "        )\n",
    "\n",
    "sql_out.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path hdfs://localhost:9000/user/jovyan/sql_result already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# write results\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m (\u001b[43msql_out\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat_ws\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msql_out\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msql_result\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlineSep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:911\u001b[0m, in \u001b[0;36mDataFrameWriter.text\u001b[0;34m(self, path, compression, lineSep)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Saves the content of the DataFrame in a text file at the specified path.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;124;03mThe text files will be encoded as UTF-8.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;124;03mEach row becomes a new line in the output file.\u001b[39;00m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression, lineSep\u001b[38;5;241m=\u001b[39mlineSep)\n\u001b[0;32m--> 911\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path hdfs://localhost:9000/user/jovyan/sql_result already exists."
     ]
    }
   ],
   "source": [
    "# write results\n",
    "(sql_out.limit(30)\n",
    "        .select(F.concat_ws(',', *sql_out.columns))\n",
    "        .write.option('header', False)\n",
    "        .text(\"sql_result\", lineSep=\"\\t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark RDD Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read csv into rdd\n",
    "rdd = sc.textFile(\"hdfs:/data/clickstream.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('3491', '862'), 1695594151),\n",
       " (('455', '220'), 1695614507),\n",
       " (('2564', '10'), 1695620464),\n",
       " (('327', '955'), 1695637653),\n",
       " (('2118', '520'), 1695644046)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the values\n",
    "rdd1 = rdd.map(lambda row: row.split('\\t'))\n",
    "# remove header\n",
    "header = rdd1.first()\n",
    "rdd2 = rdd1.filter(lambda row: row != header)\n",
    "# rdd4 contains the timestamp of first error for user_id + session_id if any\n",
    "rdd3 = rdd2.filter(lambda row: 'error' in row[2])\n",
    "rdd4 = rdd3.map(lambda row: ((row[0], row[1]), int(row[-1]))).reduceByKey(lambda a, b: min(a, b))\n",
    "rdd4.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rdd5 is our original data without header, and mapped by (user_id, session_id)\n",
    "# rdd6 join rdd5 with rdd4\n",
    "rdd5 = rdd2.map(lambda row: ((row[0], row[1]), (row[2], row[3], int(row[4]))))\n",
    "rdd6 = rdd5.leftOuterJoin(rdd4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('2218', '958'), (('page', 'main', 1695586693), None)),\n",
       " (('3491', '862'), (('page', 'main', 1695593372), 1695594151)),\n",
       " (('3491', '862'), (('page', 'internet', 1695593474), 1695594151)),\n",
       " (('3491', '862'), (('page', 'news', 1695593625), 1695594151)),\n",
       " (('3491', '862'), (('page', 'rabota', 1695593640), 1695594151))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd7 only keeps event which are 'page' from rdd6\n",
    "rdd7 = rdd6.filter(lambda row: (row[1][0][0] == 'page'))\n",
    "rdd7.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('2218', '958'), 'main'),\n",
       " (('1484', '744'), 'main'),\n",
       " (('1484', '744'), 'rabota'),\n",
       " (('1699', '297'), 'main'),\n",
       " (('1699', '297'), 'archive')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd8 filter rdd7 where there was no error in session and is mapped by (user_id, session_id) -> event_page\n",
    "rdd8 = rdd7.filter(lambda row: (row[1][1] == None)).map(lambda row: (row[0], row[1][0][1]))\n",
    "rdd8.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rdd9 filter rdd7 where there was error in session \n",
    "rdd9 = rdd7.filter(lambda row: (row[1][1] != None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('3491', '862'), 'main'),\n",
       " (('3491', '862'), 'internet'),\n",
       " (('3491', '862'), 'news'),\n",
       " (('3491', '862'), 'rabota'),\n",
       " (('3491', '862'), 'archive')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd10 removes rows from rdd9 after the error and is mapped by (user_id, session_id) -> event_page\n",
    "rdd10 = rdd9.filter(lambda row: (row[1][1] > row[1][0][2])).map(lambda row: (row[0], row[1][0][1]))\n",
    "rdd10.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('3491', '862'),\n",
       "  'main-internet-news-rabota-archive-vklad-online-bonus-news-archive-internet-archive-tariffs'),\n",
       " (('455', '220'), 'main-archive'),\n",
       " (('2564', '10'),\n",
       "  'main-tariffs-rabota-internet-rabota-archive-main-news-vklad-rabota-online-vklad-tariffs-news-online-vklad-internet'),\n",
       " (('327', '955'), 'main'),\n",
       " (('1932', '269'), 'main-main-internet-rabota-internet')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd11 takes union of rdd8 and rdd10 and concatenate pages for each session and is mapped by (user_id, session_id) -> route\n",
    "rdd11 = rdd10.union(rdd8).reduceByKey(lambda a, b: a + \"-\" + b)\n",
    "rdd11.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('main', 8090),\n",
       " ('main-archive', 1096),\n",
       " ('main-rabota', 1039),\n",
       " ('main-internet', 880),\n",
       " ('main-bonus', 865),\n",
       " ('main-news', 760),\n",
       " ('main-tariffs', 669),\n",
       " ('main-online', 583),\n",
       " ('main-vklad', 514),\n",
       " ('main-archive-rabota', 167)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd12 calculates the count of routes in descending order\n",
    "\n",
    "rdd12 = rdd11.map(lambda row: (row[1], 1)).reduceByKey(lambda a, b: a + b).sortBy(lambda row: row[1], ascending=False)\n",
    "rdd12.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'saveAsTextFile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# write results\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrdd12\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTextFile\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrdd_output\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'saveAsTextFile'"
     ]
    }
   ],
   "source": [
    "# write results\n",
    "rdd12.map(lambda row : row[0] + \"\\t\" + str(row[1])).take(30).saveAsTextFile('rdd_output')"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
